# Atlas —— 基于 RAG 的本地知识库智能问答系统

## 一、项目背景与研究意义

### 1.1 研究背景

随着大语言模型（Large Language Model, LLM）技术的快速发展，以 ChatGPT 为代表的通用对话系统展现了强大的自然语言理解与生成能力。然而，通用大模型存在以下固有局限：

1. **知识时效性问题**：模型的知识截止于训练数据的时间点，无法获取最新信息。
2. **领域知识缺失**：对于特定行业或组织的专有知识（如企业内部文档、学术论文、法规条文等），通用模型缺乏覆盖。
3. **幻觉问题（Hallucination）**：模型在缺乏相关知识时，可能生成看似合理但实际错误的回答。
4. **数据隐私顾虑**：将敏感文档上传到云端大模型服务存在数据泄露风险，不符合部分行业的合规要求。

检索增强生成（Retrieval-Augmented Generation, RAG）技术的出现为上述问题提供了有效的解决方案。RAG 通过在生成阶段引入外部知识检索环节，使大模型能够基于真实文档内容进行回答，显著提升了回答的准确性和可追溯性。

### 1.2 研究意义

本项目设计并实现了一个**完全本地化部署**的知识库智能问答系统 Atlas，具有以下研究意义：

- **理论意义**：探索 RAG 技术在本地化场景下的工程实现方案，研究文档解析、文本分块、向量检索、上下文注入等关键环节的最优实践。
- **实际意义**：为个人用户和中小型组织提供一个零成本、零依赖云端 API、保护数据隐私的本地知识库问答工具，可应用于文档助手、智能客服、研究辅助等场景。

---

## 二、系统需求分析

### 2.1 功能需求

| 编号 | 功能模块 | 功能描述 |
|------|---------|---------|
| F1 | 知识库管理 | 创建、编辑、删除知识库，支持添加描述（辅助检索理解知识库定位） |
| F2 | 文档管理 | 上传文档（PDF/DOCX/TXT/MD），自动解析、分块、向量化索引；基于 SHA-256 哈希的同知识库文件去重 |
| F3 | 文档异步处理 | 上传即时返回，后台异步完成分片和向量化，前端轮询状态展示，失败支持手动重试 |
| F4 | 文档摘要 | 上传时自动调用 LLM 生成文档摘要，摘要作为特殊片段参与向量检索，支持全局性问答 |
| F5 | 智能问答 | 基于知识库的 RAG 问答 + 不依赖知识库的通用对话，自动切换 |
| F6 | 引用溯源 | 回答附带引用来源标签（知识库名、文件名、分片编号），点击按需加载原文 |
| F7 | 思考过程 | 展示并持久化模型的思考过程（reasoning），历史消息可回显推理链路 |
| F8 | 检索增强 | 可选启用查询改写、BM25+向量混合检索（RRF 融合）、LLM 重排序、上下文扩展 |
| F9 | 对话管理 | 多轮对话历史记录、对话列表管理，按时间分组展示 |
| F10 | 模型切换 | 支持在多个本地模型间动态切换 |
| F11 | 多知识库检索 | 单次对话可同时关联多个知识库进行联合检索 |
| F12 | 文档预览 | 查看文档分片内容，了解知识被如何切分 |
| F13 | 可视化配置 | 前端设置页面修改模型、RAG 参数、检索增强策略，支持一键重置为默认值 |

### 2.2 非功能需求

| 编号 | 需求类型 | 描述 |
|------|---------|------|
| NF1 | 隐私安全 | 全部数据存储在本地，模型推理在本地完成，不依赖任何云端服务 |
| NF2 | 响应性能 | 采用流式输出（SSE），首字符响应延迟低，用户体验流畅 |
| NF3 | 一键部署 | 前后端打包为单一 macOS .dmg 安装包，用户双击即可使用 |
| NF4 | 可扩展性 | 模块化架构，文档解析器、向量数据库、LLM 均可替换 |

---

## 三、系统总体设计

### 3.1 系统架构

本系统采用**前后端分离 + 本地模型推理**的三层架构：

```
┌─────────────────────────────────────────────────────┐
│                   表现层（Presentation）               │
│   Electron 桌面应用 + React UI + Ant Design 组件库     │
│   ┌──────────┐  ┌──────────┐  ┌─────────────────┐   │
│   │ 侧边栏    │  │ 对话面板  │  │ 知识库文档管理   │   │
│   └──────────┘  └──────────┘  └─────────────────┘   │
│         │              │              │               │
│         └──────────────┼──────────────┘               │
│                        │ HTTP / SSE                   │
├────────────────────────┼────────────────────────────┤
│                   业务层（Business）                    │
│              Python FastAPI REST API                  │
│   ┌──────────┐  ┌──────────┐  ┌─────────────────┐   │
│   │ 对话 API  │  │ 文档 API  │  │ 知识库 API      │   │
│   └────┬─────┘  └────┬─────┘  └───────┬─────────┘   │
│        │              │                │              │
│   ┌────┴──────────────┴────────────────┴──────────┐  │
│   │            RAG 编排服务（核心引擎）              │  │
│   │  检索(Retrieve) → 上下文构建 → 生成(Generate)   │  │
│   └────┬──────────────┬────────────────┬──────────┘  │
│        │              │                │              │
├────────┼──────────────┼────────────────┼────────────┤
│        │         数据层（Data）          │              │
│   ┌────┴─────┐  ┌─────┴──────┐  ┌─────┴──────────┐  │
│   │ Ollama   │  │ ChromaDB   │  │ SQLite         │  │
│   │ 本地LLM  │  │ 向量数据库  │  │ 关系数据库      │  │
│   └──────────┘  └────────────┘  └────────────────┘  │
└─────────────────────────────────────────────────────┘
```

### 3.2 技术选型

| 层次 | 技术 | 版本 | 选型理由 |
|------|------|------|---------|
| **前端框架** | React | 18.2 | 组件化开发，生态成熟，适合构建复杂交互界面 |
| **UI 组件库** | Ant Design (antd) | 6.3 | 企业级 UI 组件库，提供丰富的开箱即用组件 |
| **AI 交互组件** | @ant-design/x | 2.2 | 专为 AI 对话场景设计的组件（Bubble、Think、Sender 等） |
| **状态管理** | Zustand | 4.5 | 轻量级状态管理，API 简洁，无模板代码 |
| **桌面框架** | Electron | 28.0 | 跨平台桌面应用框架，复用 Web 技术栈 |
| **应用打包** | PyInstaller | ≥6.0 | 将 Python 后端打包为独立可执行文件，嵌入 Electron 应用 |
| **构建工具** | Vite | 5.0 | 基于 ESM 的极速开发服务器和构建工具 |
| **后端框架** | FastAPI | ≥0.109 | 高性能异步 Python Web 框架，自动生成 API 文档 |
| **LLM 推理** | Ollama | ≥0.4 | 本地大模型运行平台，支持多种开源模型 |
| **对话模型** | Qwen3:14b | - | 通义千问 14B 参数模型，中文能力优秀 |
| **摘要模型** | Qwen2.5:14b | - | 文档摘要生成模型，关闭思维链输出更快 |
| **嵌入模型** | Qwen3-embedding:4b | - | 通义千问嵌入模型，中文语义表示能力强 |
| **向量数据库** | ChromaDB | ≥0.4.22 | 轻量级嵌入式向量数据库，支持持久化和相似度检索 |
| **关系数据库** | SQLite + aiosqlite | ≥2.0 | 零配置嵌入式数据库，配合异步驱动实现非阻塞访问 |
| **ORM** | SQLAlchemy | ≥2.0 | Python 主流 ORM，支持异步查询 |
| **PDF 解析** | PyMuPDF | ≥1.23 | 高性能 PDF 解析库，支持文本提取 |
| **Word 解析** | mammoth + python-docx | ≥1.8 / ≥1.1 | 主流程 mammoth 转 Markdown，失败回退 python-docx |
| **文本分块** | LangChain Text Splitters | ≥0.0.1 | 递归字符分割器，支持自定义分隔符 |
| **关键词检索** | rank_bm25 | ≥0.2.2 | BM25 关键词匹配算法，与向量语义检索互补 |
| **中文分词** | jieba | ≥0.42 | 中文分词工具，为 BM25 提供中文 tokenization |

---

## 四、系统详细设计

### 4.1 RAG 核心流程

RAG 是本系统的核心算法，其完整流程如下：

```
用户提问
  │
  ▼
┌──────────────────┐
│  1. 查询改写       │  （可选）调用 LLM 改写用户问题
│  Query Rewrite    │  模糊问题 → 明确检索语句
│                   │  复合问题 → 拆分多个子查询
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  2. 问题向量化     │  调用 Ollama Embedding 模型
│  Query → Vector   │  将自然语言问题转换为高维向量
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  3. 混合检索       │  语义检索：ChromaDB 余弦相似度 Top-K
│  Hybrid Search    │  关键词检索（可选）：BM25 + jieba 分词
│                   │  融合策略：RRF (Reciprocal Rank Fusion)
│                   │  支持按文档 ID 过滤检索范围
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  4. 重排序         │  （可选）LLM 对初筛 Top-N 结果
│  Reranking        │  按相关性打分（0-10），精选 Top-K
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  5. 上下文扩展     │  自动获取命中片段的相邻片段
│  Context Expand   │  chunk_index ± 1，保证上下文完整
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  6. 层级化上下文   │  知识库 description → 注入 system prompt
│  构建             │  文档 summary → 优先展示全局概要
│  Context Build    │  内容 chunk → 具体细节
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  7. 提示词组装     │  将上下文注入到系统提示词模板中
│  Context → Prompt │  与对话历史记录一起构建完整的消息数组
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  8. 流式生成       │  调用 Ollama 对话模型（Qwen3:14b）
│  Prompt → Stream  │  通过 SSE 逐 token 推送到前端
│  Response         │  分离「思考过程」和「回答内容」
│                   │  流结束后附带引用来源信息
└──────────────────┘
```

### 4.2 文档索引流程

文档从上传到可被检索，经历以下处理管线：

```
用户上传文件
  │
  ▼
┌──────────────────┐
│ 1. 文件去重        │  计算文件 SHA-256 哈希
│  Hash Check       │  同知识库内检查是否已存在相同文件
│                   │  重复则拒绝上传并提示
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 2. 快速返回        │  保存文件到 data/uploads/
│  Save & Return    │  创建 Document 记录（status=pending）
│                   │  即时返回文档 ID，前端开始轮询状态
└────────┬─────────┘
         │ 异步后台任务
         ▼
┌──────────────────┐
│ 3. 文档解析        │  根据文件类型选择解析器（策略模式）
│  PDF  → PyMuPDF   │  PDF:     逐页提取文本
│  DOCX → mammoth   │  DOCX:    先转 Markdown，失败回退 python-docx
│    (fallback:     │  TXT/MD:  自动检测编码后读取
│     python-docx)  │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 4. 文本分块        │  RecursiveCharacterTextSplitter
│  chunk_size=600   │  按语义边界递归分割
│  overlap=100      │  相邻片段保留 100 字符重叠
│  min_chars=60     │  过短分片自动并入相邻分片
│                   │  分隔符优先级：段落 > 句子 > 逗号 > 字符
│                   │  针对中英文混合内容优化
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 5. 向量化          │  调用 Ollama Embedding 模型
│  Text → Vector    │  批量将所有片段转换为向量表示
│                   │  自动按 50 个一批分批发送，避免大文件超时
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 6. 摘要生成        │  调用 LLM 对全文生成文档摘要
│  Text → Summary   │  摘要存入 SQLite 的 summary 字段
│                   │  同时作为特殊片段（type=summary）存入 ChromaDB
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 7. 持久化存储      │  向量 + 原文 + 元数据 → ChromaDB
│  status=completed │  文档元信息 → SQLite
│                   │  失败则标记 status=failed，可手动重试
└──────────────────┘
```

### 4.3 文本分块策略

文本分块是 RAG 系统中影响检索质量的关键环节。本系统采用递归字符分割策略，针对中英文混合文档优化了分隔符优先级：

| 优先级 | 分隔符 | 说明 |
|-------|--------|------|
| 1 | `\n\n` | 段落分隔（最优分割点） |
| 2 | `\n` | 换行 |
| 3 | `。！？；` | 中文句末标点 |
| 4 | `. ! ? ;` | 英文句末标点 |
| 5 | `，` `, ` | 逗号 |
| 6 | 空格 | 词级分割 |
| 7 | 空字符串 | 字符级兜底 |

分块参数设计依据：
- **chunk_size = 600 字符**：平衡语义完整性和检索精度。过大导致检索不精确，过小导致语义断裂。
- **chunk_overlap = 100 字符**：相邻片段保留重叠区域，确保跨分块边界的信息不被丢失。
- **chunk_min_chars = 60 字符**：避免出现过短、噪声分片，提升检索质量。

### 4.4 数据库设计

#### 4.4.1 关系数据库（SQLite）—— E-R 图

```
┌──────────────────┐        ┌──────────────────┐
│  KnowledgeBase   │        │   Conversation   │
│──────────────────│        │──────────────────│
│ PK  id (UUID)    │        │ PK  id (UUID)    │
│     name         │        │     title        │
│     description  │        │     created_at   │
│     created_at   │        │     updated_at   │
│     updated_at   │        └────────┬─────────┘
└────────┬─────────┘                 │
         │ 1:N                       │ 1:N
         │                           │
┌────────┴─────────┐        ┌────────┴─────────┐
│    Document      │        │     Message      │
│──────────────────│        │──────────────────│
│ PK  id (UUID)    │        │ PK  id (UUID)    │
│ FK  kb_id        │        │ FK  conv_id      │
│     filename     │        │     role         │
│     file_type    │        │     content      │
│     file_size    │        │     reasoning    │
│     file_hash    │        │     references   │
│     chunk_count  │        │     created_at   │
│     summary      │        └──────────────────┘
│     status       │
│     created_at   │        ┌──────────────────┐
└──────────────────┘        │     Setting      │
                            │──────────────────│
                            │ PK  key          │
                            │     value        │
                            │     updated_at   │
                            └──────────────────┘
```

#### 4.4.2 向量数据库（ChromaDB）

| 字段 | 类型 | 说明 |
|------|------|------|
| id | string | 片段唯一标识（格式：`{document_id}_{chunk_index}`） |
| embedding | float[] | 文本向量表示 |
| document | string | 片段原文内容 |
| metadata | dict | 元数据（document_id, filename, chunk_index, total_chunks） |

检索方式：余弦相似度（cosine similarity），返回 Top-K 最相似片段。

### 4.5 前端界面设计

系统界面采用经典的「侧边栏 + 主内容区」双栏布局：

```
┌──────────┬──────────────────────────────────┐
│          │       模型选择下拉菜单             │
│  搜索框   ├──────────────────────────────────┤
│          │                                   │
│ ┌──────┐ │         对话消息气泡区域            │
│ │对话1  │ │                                   │
│ │对话2  │ │   [用户消息]          蓝色气泡右对齐│
│ │对话3  │ │   [思考过程折叠面板]               │
│ │ ...  │ │   [助手回答] Markdown 渲染 左对齐  │
│ └──────┘ │   [复制按钮]                       │
│          │                                   │
│          ├──────────────────────────────────┤
│ ┌──────┐ │   [已选知识库标签]                  │
│ │ 对话  │ │   ┌─────────────────────────────┐ │
│ │知识库 │ │   │ 📚 消息输入框          发送  │ │
│ └──────┘ │   └─────────────────────────────┘ │
└──────────┴──────────────────────────────────┘
  侧边栏                 主内容区
```

侧边栏支持「对话」和「知识库」两种模式切换：
- **对话模式**：展示对话历史列表，按时间分组（今天/昨天/7天内/30天内/更早）
- **知识库模式**：展示知识库列表，支持勾选（用于对话检索范围）和点击（查看文档管理）

### 4.6 前后端通信设计

| 通信方式 | 应用场景 | 说明 |
|---------|---------|------|
| REST API (JSON) | CRUD 操作 | 知识库/文档/对话的增删改查 |
| SSE (Server-Sent Events) | 流式对话 | 模型生成过程实时推送到前端 |
| multipart/form-data | 文件上传 | 文档上传 |

SSE 流式通信的数据格式：
```
data: {"content": "回答片段", "reasoning": "思考过程片段", "done": false}\n\n
data: {"content": "", "reasoning": "", "done": true, "references": [{"knowledge_base_name": "...", "filename": "...", "chunk_index": 0, "distance": 0.23}]}\n\n
```

---

## 五、系统核心模块实现

### 5.1 后端模块划分

```
backend/app/
├── main.py              # FastAPI 应用入口（生命周期管理、中间件、路由注册）
├── config.py            # 全局配置（路径、模型、RAG 参数，支持前端可视化修改、.env 覆盖）
├── api/                 # API 路由层
│   ├── chat.py          # 对话接口（SSE 流式响应、对话 CRUD、引用溯源）
│   ├── documents.py     # 文档接口（异步上传、去重、状态查询、重试、分片按需加载）
│   ├── knowledge_bases.py  # 知识库接口（CRUD、级联删除）
│   ├── settings.py      # 配置管理接口（查询、修改、重置）
│   └── history.py       # 历史消息查询接口
├── core/                # 核心业务逻辑层
│   ├── rag.py           # RAG 编排服务（查询改写、混合检索、重排序、上下文扩展、层级化构建）
│   ├── vectorstore.py   # ChromaDB 向量存储封装（增删查、BM25 语料获取、相邻片段）
│   ├── ollama.py        # Ollama 模型服务封装（对话 + 向量化）
│   ├── parser.py        # 多格式文档解析（策略模式）
│   └── chunker.py       # 文本分块（递归字符分割，中英文优化）
├── models/              # ORM 数据模型
│   └── models.py        # KnowledgeBase, Document, Conversation, Message, Setting
└── db/                  # 数据库访问层
    ├── database.py      # SQLAlchemy 异步引擎、会话管理、增量迁移
    └── crud.py          # CRUD 操作封装
```

### 5.2 前端模块划分

```
frontend/src/
├── main/index.js              # Electron 主进程（窗口管理、后端进程生命周期）
├── preload/preload.js         # 预加载脚本（安全桥接、后端端口传递）
└── renderer/                  # React 渲染进程
    ├── App.tsx                # 根组件（布局和视图切换）
    ├── components/
    │   ├── ChatPanel.tsx      # 对话面板（消息收发、流式渲染、思考过程、引用溯源展示）
    │   ├── Sidebar.tsx        # 侧边栏（对话列表 / 知识库列表，知识库创建编辑 Modal）
    │   ├── KnowledgeBaseView.tsx  # 知识库文档管理（上传/预览/删除/状态展示/重试）
    │   └── SettingsPage.tsx   # 配置页面（分组展示、编辑、保存、重置）
    ├── services/
    │   └── api.ts             # API 服务层（HTTP 请求 + SSE 流式解析 + 配置/引用/重试 API）
    ├── stores/
    │   ├── conversationStore.ts  # 对话状态管理（Zustand）
    │   └── knowledgeBaseStore.ts # 知识库状态管理（Zustand，含轮询刷新）
    └── styles/
        └── global.css         # 全局样式
```

### 5.3 设计模式应用

| 设计模式 | 应用位置 | 说明 |
|---------|---------|------|
| **策略模式** | 文档解析器 (parser.py) | BaseParser 定义接口，PDFParser/DocxParser/TextParser 各自实现，DocumentParser 统一调度 |
| **单例模式** | 核心服务实例 | rag_service、vector_store、ollama_service 均为全局单例 |
| **观察者模式** | SSE 流式通信 | 后端推送事件，前端注册回调处理 |
| **依赖注入** | FastAPI 路由 | 通过 Depends(get_session) 注入数据库会话 |
| **状态管理模式** | Zustand Store | 集中管理前端应用状态，组件通过 Hook 订阅 |

---

## 六、系统特色与创新点

### 6.1 完全本地化部署

系统全部组件运行在用户本机，不依赖任何云端服务：
- **模型推理**：通过 Ollama 在本地运行开源大模型
- **向量检索**：ChromaDB 嵌入式向量数据库
- **数据存储**：SQLite 嵌入式关系数据库
- **文件存储**：本地文件系统

这确保了用户数据的完全隐私，适用于对数据安全有严格要求的场景。

### 6.2 双模式智能切换

系统根据用户是否选择知识库，自动在两种模式间切换：
- **RAG 模式**：选择了知识库时，先检索再生成，回答基于文档内容
- **通用对话模式**：未选择知识库时，直接与大模型对话

用户无需手动切换，系统根据上下文自动选择最合适的模式。

### 6.3 多策略检索增强

系统实现了多层次的检索增强策略，全部可通过前端设置页面按需启用：

- **查询改写（Query Rewriting）**：调用 LLM 将用户的模糊/口语化问题改写为更适合语义检索的查询语句，复合问题自动拆分为多个子查询分别检索
- **混合检索（Hybrid Search）**：向量语义检索 + BM25 关键词检索双路并行，使用 RRF（Reciprocal Rank Fusion）加权融合两路结果，兼顾语义理解和精确关键词匹配
- **LLM 重排序（Reranking）**：初筛 Top-N 结果后，调用 LLM 按相关性对每条结果打分（0-10），精选 Top-K 送入生成
- **上下文扩展（Context Expansion）**：命中片段自动获取相邻片段（chunk_index ± 1），保证上下文完整性
- **层级化检索**：利用知识库 description → 文档 summary → 内容 chunk 三层信息构建层级化上下文

### 6.4 文档摘要与全局性问答

文档上传后自动调用 LLM 生成摘要，摘要同时作为特殊片段（metadata type=summary）存入向量库参与检索。当用户提出"请介绍一下这份文档"等全局性问题时，摘要片段优先被召回，解决了传统 RAG 只能做点对点片段检索的局限。

### 6.5 引用溯源

RAG 回答附带引用来源标签，显示知识库名、文件名、分片编号和语义距离。引用信息仅存储轻量索引（不含原文），用户点击时按需从 ChromaDB 查询分片内容，再次点击同一引用可收起。摘要分片支持通过 `chunk_index=-1` 按需查询。引用信息持久化到消息的 references 字段，历史消息回显时仍可查看。

### 6.6 文档异步处理与去重

- **异步处理**：上传接口只做文件保存和记录创建即时返回，分片和向量化通过 `asyncio.create_task` 在后台异步执行。前端根据文档状态（pending/processing/completed/failed）展示不同 UI，3 秒间隔自动轮询刷新
- **文件去重**：上传时计算 SHA-256 哈希，同知识库内相同文件拒绝重复上传
- **失败重试**：分片失败的文档可通过 reindex 接口手动触发重新处理

### 6.7 思考过程可视化

系统支持展示模型的「思考过程」（reasoning）——模型在给出最终回答前的推理链路。前端通过可折叠面板呈现，帮助用户理解模型的推理逻辑，增强回答的可解释性。思考过程持久化存储，加载历史消息时可完整回显。

### 6.8 可视化配置管理

前端设置页面分组展示所有可配置项（模型配置、RAG 参数、对话参数、检索增强），支持实时修改并即时生效（无需重启）。配置持久化到 SQLite 的 settings 表，应用重启后自动加载用户自定义配置。每个配置项显示当前值和默认值，支持一键重置。

### 6.9 中英文混合分块优化

文本分块器针对中英文混合文档定制了分隔符优先级，兼顾中文标点（。！？；，）和英文标点（. ! ? ; ,），确保在混合语言场景下的分块质量。

### 6.10 多知识库联合检索

支持在单次对话中同时关联多个知识库，系统将跨知识库进行联合语义检索，适用于需要综合多个文档来源的复杂问答场景。

### 6.11 流式响应体验

采用 SSE（Server-Sent Events）实现流式输出，模型每生成一个 token 即推送到前端渲染，首字符延迟低，用户无需等待完整回答生成即可开始阅读。

### 6.12 一键打包部署

系统支持将前后端打包为单一 macOS .dmg 安装包，用户双击安装后即可使用：
- **后端打包**：使用 PyInstaller 将 Python FastAPI 后端及全部依赖（ChromaDB、PyMuPDF 等）打包为独立可执行文件，嵌入 Electron 应用的 Resources 目录
- **动态端口分配**：Electron 主进程启动时自动寻找可用端口，避免端口冲突
- **进程生命周期管理**：应用启动时自动拉起后端进程并轮询 `/health` 确认就绪；退出时先发 SIGTERM 优雅关闭，5 秒超时后 SIGKILL 强制终止
- **环境自适应配置**：通过 `model_validator` 动态计算数据目录——打包模式使用 `~/Library/Application Support/Atlas/data/`，开发模式使用项目根目录下的 `data/`
- **开发体验不受影响**：所有打包相关逻辑通过 `app.isPackaged` 条件判断，开发模式完全保持原有行为

### 6.13 大文件向量化分批处理

对大文档产生的数百个文本片段，自动按每批 50 个分批发送给 Ollama Embedding 模型进行向量化，避免单次请求量过大导致超时或内存溢出。

---

## 七、系统运行环境

### 7.1 硬件要求

| 项目 | 最低要求 | 推荐配置 |
|------|---------|---------|
| CPU | 4 核 | 8 核以上 |
| 内存 | 16 GB | 32 GB |
| 显存 | 8 GB (用于 GPU 推理) | 16 GB 以上 |
| 磁盘 | 20 GB 可用空间 | SSD 50 GB 以上 |

> 注：如仅使用 CPU 推理，无显卡要求，但推理速度较慢。

### 7.2 软件环境

| 软件 | 版本要求 |
|------|---------|
| 操作系统 | Windows 10+、macOS 12+、Linux |
| Node.js | ≥ 18.0 |
| Python | ≥ 3.9 |
| Ollama | 最新版 |

### 7.3 模型要求

| 模型 | 用途 | 参数量 | 磁盘占用 |
|------|------|-------|---------|
| qwen3:14b | 对话生成 | 14B | ~8 GB |
| qwen2.5:14b | 文档摘要生成 | 14B | ~8 GB |
| qwen3-embedding:4b | 文本向量化 | 4B | ~2.5 GB |

---

## 八、项目目录结构总览

```
Atlas/
├── frontend/                     # 前端工程（Electron + React）
│   ├── src/
│   │   ├── main/index.js         # Electron 主进程入口（含后端进程管理）
│   │   ├── preload/preload.js    # 预加载安全桥接脚本（含后端端口传递）
│   │   └── renderer/             # React 渲染进程
│   │       ├── App.tsx           # 根组件
│   │       ├── main.tsx          # React 入口
│   │       ├── components/       # UI 组件（ChatPanel, Sidebar, KnowledgeBaseView, SettingsPage）
│   │       ├── services/api.ts   # API 服务层（动态 API 地址，含配置/引用/重试 API）
│   │       ├── stores/           # Zustand 状态管理
│   │       ├── types/electron.d.ts # Electron API 类型声明
│   │       └── styles/           # CSS 样式
│   ├── build/
│   │   └── entitlements.mac.plist # macOS 权限声明（网络、JIT、动态库加载）
│   ├── index.html                # HTML 模板
│   ├── package.json              # 依赖与脚本配置（含 electron-builder 打包配置）
│   ├── tsconfig.json             # TypeScript 配置
│   └── vite.config.ts            # Vite 构建配置
│
├── backend/                      # 后端工程（Python FastAPI）
│   ├── app/
│   │   ├── main.py               # FastAPI 应用入口
│   │   ├── config.py             # 全局配置（支持环境变量覆盖、前端可视化修改、打包模式自适应）
│   │   ├── api/                  # REST API 路由（chat, documents, knowledge_bases, settings, history）
│   │   ├── core/                 # 核心逻辑（rag, vectorstore, ollama, parser, chunker）
│   │   ├── models/models.py      # SQLAlchemy ORM 模型（5 张表）
│   │   └── db/                   # 数据库（engine, session, CRUD, 增量迁移）
│   ├── run.py                    # PyInstaller 入口脚本
│   ├── atlas-backend.spec        # PyInstaller 打包配置
│   └── requirements.txt          # Python 依赖
│
├── data/                         # 运行时数据（自动生成）
│   ├── chroma/                   # ChromaDB 向量库持久化
│   ├── sqlite/atlas.db           # SQLite 关系数据库
│   └── uploads/                  # 用户上传的文档文件
│
├── build.sh                      # 一键构建脚本（PyInstaller → Vite → electron-builder）
├── TODO.md                       # 待办事项
└── README.md                     # 项目说明
```

---

## 九、API 接口汇总

### 9.1 知识库管理

| 方法 | 路径 | 说明 |
|------|------|------|
| POST | `/api/knowledge-bases` | 创建知识库 |
| GET | `/api/knowledge-bases` | 获取知识库列表 |
| GET | `/api/knowledge-bases/{id}` | 获取知识库详情（含文档列表） |
| PUT | `/api/knowledge-bases/{id}` | 更新知识库信息 |
| DELETE | `/api/knowledge-bases/{id}` | 删除知识库（级联删除文档和向量） |

### 9.2 文档管理

| 方法 | 路径 | 说明 |
|------|------|------|
| POST | `/api/documents/upload` | 上传文档（异步处理，即时返回，含文件去重） |
| GET | `/api/documents` | 获取文档列表（支持按知识库过滤，含处理状态） |
| GET | `/api/documents/{id}` | 获取文档详情（含分片内容） |
| GET | `/api/documents/{id}/chunks/{index}` | 获取单个分片内容（按需加载，用于引用溯源；`index=-1` 返回摘要） |
| POST | `/api/documents/{id}/reindex` | 重新分片处理（用于失败文档重试） |
| DELETE | `/api/documents/{id}` | 删除文档（同时清理向量和文件） |

### 9.3 对话管理

| 方法 | 路径 | 说明 |
|------|------|------|
| POST | `/api/chat` | 发送消息（SSE 流式响应，含引用溯源信息） |
| POST | `/api/chat/conversations` | 创建新对话 |
| GET | `/api/chat/conversations` | 获取对话列表 |
| GET | `/api/chat/conversations/{id}` | 获取对话详情（含消息记录、思考过程、引用信息） |
| PUT | `/api/chat/conversations/{id}` | 更新对话标题 |
| DELETE | `/api/chat/conversations/{id}` | 删除对话 |
| GET | `/api/chat/models` | 获取可用模型列表 |

### 9.4 配置管理

| 方法 | 路径 | 说明 |
|------|------|------|
| GET | `/api/settings` | 获取当前生效配置及默认值 |
| PUT | `/api/settings` | 修改配置项（即时生效） |
| POST | `/api/settings/reset` | 重置全部或指定配置项为默认值 |

### 9.5 历史记录

| 方法 | 路径 | 说明 |
|------|------|------|
| GET | `/api/history/messages/{conversation_id}` | 获取对话消息历史 |

---

## 十、总结

本项目设计并实现了 Atlas —— 一个基于 RAG 技术的本地知识库智能问答系统。系统采用 Electron + React 构建跨平台桌面客户端，Python FastAPI 提供后端 API 服务，Ollama 驱动本地大模型推理，ChromaDB 实现语义向量检索，SQLite 管理结构化数据。

系统完整实现了从文档上传（异步处理、文件去重）、解析、分块、向量化索引、摘要生成，到多策略检索增强（查询改写、BM25+向量混合检索、LLM 重排序、上下文扩展）、层级化上下文构建、流式生成回答、引用溯源的全链路 RAG 流程。同时提供了思考过程可视化、可视化配置管理等辅助功能。

通过完全本地化的部署方式，在保证数据隐私的同时，提供了流畅的对话交互体验。系统支持一键打包为 macOS .dmg 安装包（使用 PyInstaller 打包后端、electron-builder 打包前端），用户双击安装即可使用，无需手动配置开发环境。

项目代码结构清晰、模块化程度高，核心组件（文档解析器、向量数据库、LLM 服务、检索策略）均采用接口抽象，具有良好的可扩展性，为后续替换模型、扩展文档格式、优化检索策略提供了便利的扩展基础。
